#!/bin/bash
#
# Show available LaCT ASR training options
#

# Color codes
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
BOLD='\033[1m'
NC='\033[0m'

echo -e "${BOLD}LaCT ASR Training Options${NC}"
echo "========================="
echo ""

echo -e "${GREEN}üöÄ Quick Start (Recommended)${NC}"
echo "Complete automated setup and training:"
echo ""
echo -e "${BLUE}For HPC/SLURM systems:${NC}"
echo "  sbatch ./scripts/setup_and_train.sh"
echo ""
echo -e "${BLUE}For local systems:${NC}"
echo "  ./scripts/setup_and_train_local.sh"
echo ""
echo "This will:"
echo "  ‚Ä¢ Download LibriSpeech dataset (~7GB, 100 hours)"
echo "  ‚Ä¢ Install missing dependencies"
echo "  ‚Ä¢ Start training with optimal settings"
echo "  ‚Ä¢ Automatically resume if interrupted"
echo ""

echo -e "${GREEN}üìä Dataset Size Options${NC}"
echo "Choose based on your needs and resources:"
echo ""
echo -e "${BLUE}Minimal (Testing/Development):${NC}"
echo "  # HPC: sbatch ./scripts/setup_and_train.sh --dataset-size minimal"
echo "  # Local: ./scripts/setup_and_train_local.sh --dataset-size minimal"
echo "  ‚Ä¢ Dataset: train-clean-100 + dev-clean (~100 hours)"
echo "  ‚Ä¢ Download: ~7GB"
echo "  ‚Ä¢ Training time: ~8-12 hours (GPU dependent)"
echo "  ‚Ä¢ Expected WER: ~12-15%"
echo ""
echo -e "${BLUE}Standard (Full Training):${NC}"
echo "  # HPC: sbatch ./scripts/setup_and_train.sh --dataset-size standard"
echo "  # Local: ./scripts/setup_and_train_local.sh --dataset-size standard"
echo "  ‚Ä¢ Dataset: train-clean-360 + dev-clean (~360 hours)"
echo "  ‚Ä¢ Download: ~24GB"
echo "  ‚Ä¢ Training time: ~24-48 hours"
echo "  ‚Ä¢ Expected WER: ~8-12%"
echo ""
echo -e "${BLUE}Full (Research/Best Performance):${NC}"
echo "  # HPC: sbatch ./scripts/setup_and_train.sh --dataset-size full"
echo "  # Local: ./scripts/setup_and_train_local.sh --dataset-size full"
echo "  ‚Ä¢ Dataset: All clean subsets (~500+ hours)"
echo "  ‚Ä¢ Download: ~30GB"
echo "  ‚Ä¢ Training time: ~48-72 hours"
echo "  ‚Ä¢ Expected WER: ~6-10%"
echo ""

echo -e "${GREEN}‚öôÔ∏è Custom Training Parameters${NC}"
echo "Fine-tune training for your hardware:"
echo ""
echo -e "${BLUE}High-end GPU (24GB+ VRAM):${NC}"
echo "  ./scripts/setup_and_train.sh \\"
echo "    --dataset-size standard \\"
echo "    --batch-size 16 \\"
echo "    --epochs 30"
echo ""
echo -e "${BLUE}Mid-range GPU (8-16GB VRAM):${NC}"
echo "  ./scripts/setup_and_train.sh \\"
echo "    --dataset-size minimal \\"
echo "    --batch-size 8 \\"
echo "    --epochs 25"
echo ""
echo -e "${BLUE}Lower-end GPU (4-8GB VRAM):${NC}"
echo "  ./scripts/setup_and_train.sh \\"
echo "    --dataset-size minimal \\"
echo "    --batch-size 4 \\"
echo "    --epochs 20"
echo ""

echo -e "${GREEN}üîÑ Resume Training${NC}"
echo "For HPC or interrupted training:"
echo ""
echo -e "${BLUE}After HPC job timeout:${NC}"
echo "  # Just run the same command again - it will auto-resume"
echo "  ./scripts/setup_and_train.sh --skip-download --data-dir /path/to/data"
echo ""
echo -e "${BLUE}Manual resume:${NC}"
echo "  ./examples/train_librispeech.sh \\"
echo "    --data-dir /path/to/LibriSpeech \\"
echo "    --output-dir ./checkpoints/existing_run"
echo ""

echo -e "${GREEN}üõ†Ô∏è Manual Setup${NC}"
echo "For advanced users or custom workflows:"
echo ""
echo -e "${BLUE}1. Download dataset only:${NC}"
echo "  ./scripts/download_librispeech.sh --data-dir /custom/path"
echo ""
echo -e "${BLUE}2. Train with existing dataset:${NC}"
echo "  ./examples/train_librispeech.sh --data-dir /custom/path/LibriSpeech"
echo ""
echo -e "${BLUE}3. Custom subsets:${NC}"
echo "  ./scripts/download_librispeech.sh \\"
echo "    --subsets \"train-clean-360 train-other-500 dev-clean\""
echo ""

echo -e "${GREEN}üìã Pre-flight Checklist${NC}"
echo "Before starting training:"
echo ""
echo "‚úì Check GPU memory:"
echo "  nvidia-smi"
echo ""
echo "‚úì Check disk space (need ~30GB+ for full dataset):"
echo "  df -h"
echo ""
echo "‚úì Verify Python environment:"
echo "  python -c \"import torch; print(torch.cuda.is_available())\""
echo ""
echo "‚úì Optional - set up virtual environment:"
echo "  python -m venv lact_env"
echo "  source lact_env/bin/activate"
echo ""

echo -e "${GREEN}üéØ Monitoring Training${NC}"
echo "Track your training progress:"
echo ""
echo "‚Ä¢ Checkpoints saved to: ./checkpoints/librispeech_base/"
echo "‚Ä¢ Best model: ./checkpoints/librispeech_base/best_model.pt"
echo "‚Ä¢ Training logs: Check terminal output"
echo "‚Ä¢ Resume: Scripts automatically detect and resume from latest checkpoint"
echo ""

echo -e "${GREEN}üÜò Troubleshooting${NC}"
echo "Common issues and solutions:"
echo ""
echo -e "${YELLOW}Out of GPU memory:${NC}"
echo "  ‚Ä¢ Reduce batch size: --batch-size 4"
echo "  ‚Ä¢ Use gradient accumulation (automatic)"
echo ""
echo -e "${YELLOW}Download failures:${NC}"
echo "  ‚Ä¢ Resume: ./scripts/download_librispeech.sh --force"
echo "  ‚Ä¢ Check internet connection"
echo ""
echo -e "${YELLOW}Training crashes:${NC}"
echo "  ‚Ä¢ Check CUDA compatibility"
echo "  ‚Ä¢ Verify dataset integrity: run test script in dataset directory"
echo ""

echo -e "${GREEN}üìö More Information${NC}"
echo "‚Ä¢ Full documentation: ./scripts/README.md"
echo "‚Ä¢ Model details: ./README.md"
echo "‚Ä¢ Configuration options: ./configs/"
echo ""

echo -e "${BOLD}Ready to start? Choose an option above! üöÄ${NC}"
